{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of the TODO's and/or use more than one cell to complete all the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import IPython\n",
    "from PIL import Image\n",
    "\n",
    "# AWS\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "## SM\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tuner import (\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    "    IntegerParameter,\n",
    ")\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    rule_configs,\n",
    "    ProfilerRule,\n",
    "    DebuggerHookConfig,\n",
    "    CollectionConfig,\n",
    "    ProfilerConfig,\n",
    "    FrameworkProfile,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "**TODO:** Run the cell below to download the data.\n",
    "\n",
    "The cell below creates a folder called `train_data`, downloads training data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder. For instance, all images in folder `1` has images with 1 object in them. Images are not divided into training, testing or validation sets. If you feel like the number of samples are not enough, you can always download more data (instructions for that can be found [here](https://registry.opendata.aws/amazon-bin-imagery/)). However, we are not acessing you on the accuracy of your final trained model, but how you create your machine learning engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def download_and_arrange_data():\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "\n",
    "    with open(\"file_list.json\", \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    for k, v in d.items():\n",
    "        print(f\"Downloading Images with {k} objects\")\n",
    "        directory = os.path.join(\"train_data\", k)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        for file_path in tqdm(v):\n",
    "            file_name = os.path.basename(file_path).split(\".\")[0] + \".jpg\"\n",
    "            s3_client.download_file(\n",
    "                \"aft-vbi-pds\",\n",
    "                os.path.join(\"bin-images\", file_name),\n",
    "                os.path.join(directory, file_name),\n",
    "            )\n",
    "\n",
    "\n",
    "download_and_arrange_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "**TODO:** Explain what dataset you are using for this project. Give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understanding of it. You can find more information about the data [here](https://registry.opendata.aws/amazon-bin-imagery/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.upload_data(path=\"cap\", bucket=bucket, key_prefix=\"cap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "**TODO:** This is the part where you can train a model. The type or architecture of the model you use is not important. \n",
    "\n",
    "**Note:** You will need to use the `train.py` script to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "hyperparameters = {\"epochs\": 10, \"batch_size\": 32, \"learning_rate\": 0.001}\n",
    "\n",
    "# Create training estimator\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"pytorch\",\n",
    "    session.boto_region_name,\n",
    "    version=\"1.10\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    ")\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f\"s3://{bucket}/model_output/\",\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "estimator.fit({\"train\": f\"s3://{bucket}/train_data\"}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standout Suggestions\n",
    "You do not need to perform the tasks below to finish your project. However, you can attempt these tasks to turn your project into a more advanced portfolio piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "**TODO:** Here you can perform hyperparameter tuning to increase the performance of your model. You are encouraged to \n",
    "- tune as many hyperparameters as you can to get the best performance from your model\n",
    "- explain why you chose to tune those particular hyperparameters and the ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SM_CHANNEL_TRAIN\"] = f\"s3://{bucket}/cap/\"\n",
    "os.environ[\"SM_MODEL_DIR\"] = f\"s3://{bucket}/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch_size\": CategoricalParameter([16, 32, 64]),\n",
    "    \"epochs\": IntegerParameter(2, 6),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"hpo.py\",\n",
    "    base_job_name=\"cap-class\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.5.1\",\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type=objective_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\"train\": os.environ[\"SM_CHANNEL_TRAIN\"]}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = tuner.best_estimator()\n",
    "best_hyp = best_estimator.hyperparameters()\n",
    "\n",
    "best_train_hyp = {}\n",
    "for key in hyperparameter_ranges:\n",
    "    best_train_hyp[key] = best_hyp[key]\n",
    "best_train_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling and Debugging\n",
    "**TODO:** Use model debugging and profiling to better monitor and debug your model training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(num_steps=10),\n",
    ")\n",
    "\n",
    "debugger_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"1\", \"eval.save_interval\": \"10\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"train_model.py\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.5.1\",\n",
    "    hyperparameters=best_train_hyp,\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_config,\n",
    "    rules=rules,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\"train\": os.environ[\"SM_CHANNEL_TRAIN\"]}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "\n",
    "print(f\"Jobname: {job_name}\")\n",
    "print(f\"Client: {client}\")\n",
    "print(f\"Description: {description}\")\n",
    "\n",
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print('Profiler report location: {}'.format(rule_output_path))\n",
    "\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_artifacts_path = \"s3://sagemaker-us-east-1-934421875319/pytorch-training-2025-03-13-20-52-24-429/debug-output/\"\n",
    "\n",
    "trial = create_trial(debug_artifacts_path)\n",
    "\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "print(profiler_report_name)\n",
    "IPython.display.HTML(\n",
    "    filename=profiler_report_name + \"/profiler-output/profiler-report.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deploying and Querying\n",
    "**TODO:** Can you deploy your model to an endpoint and then query that endpoint to get a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = estimator.model_data\n",
    "model_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_serializer = sagemaker.serializers.IdentitySerializer(\"image/jpeg\")\n",
    "json_deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "\n",
    "class ImgPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(ImgPredictor, self).__init__(\n",
    "            endpoint_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            serializer=jpeg_serializer,\n",
    "            deserializer=json_deserializer,\n",
    "        )\n",
    "\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_location,\n",
    "    role=role,\n",
    "    entry_point=\"cap_endpoint.py\",\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.5.1\",\n",
    "    predictor_cls=ImgPredictor,\n",
    ")\n",
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = \"nd009t-capstone-starter/starter/train_data/2/00113.jpg\"\n",
    "\n",
    "with open(test_image, \"rb\") as f:\n",
    "    payload = f.read()\n",
    "    display(Image.open(io.BytesIO(payload)))\n",
    "    response = predictor.predict(payload, initial_args={\"ContentType\": \"image/jpeg\"})\n",
    "    prediction = np.argmax(response, 1) + 1\n",
    "\n",
    "    print(f\"Class: 118\")\n",
    "    print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaper Training and Cost Analysis\n",
    "**TODO:** Can you perform a cost analysis of your system and then use spot instances to lessen your model training cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600,\n",
    "    max_run=1800,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "spot_estimator.fit({\"train\": f\"s3://{bucket}/train_data\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Instance Training\n",
    "**TODO:** Can you train your model on multiple instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600,\n",
    "    max_run=1800,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "spot_estimator.fit({\"train\": f\"s3://{bucket}/train_data\"})"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
